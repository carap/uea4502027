{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "# INTRODUCTION TO MACHINE LEARNING\n",
    "---\n",
    "## Linear Regression\n",
    "\n",
    "- Email: <cross224@hotmail.com>\n",
    "- GitHub: [@carap](https://github.com/carap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "# MACHINE LEARNING\n",
    "---\n",
    "# General workflow (*pipeline*) \n",
    "Most projects can be thought of as a series of discrete steps:\n",
    "* Data acquisition/loading and preprocessing\n",
    "* Feature creation, selection, and normalization\n",
    "* Model building (multiple models combined) and testing\n",
    "* Reporting/Deployment\n",
    "![alt text](./images/ml-process.svg \"Machine Learning process\")\n",
    "\n",
    "> ***NOTE***: We've firstly worked with **DATA**, but now we have to deal with **MODELS**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning \n",
    "Broad notion of building **computational structures** that progressively ***learn*** based on **experience**\n",
    "+ In fact, it is not just about building this *structures*, but the ***math, science, engineering, stats, and the computing*** behind these structures for gleaning **information** from historical data sets, in order to process new data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Modelling\n",
    "\n",
    "A very **interdisciplinary field**...\n",
    "- Cognitive Neuroscience and Artificial Intelligence \n",
    "- Information and Control theory\n",
    "- Stochastic signal processing, and Statistical Learning \n",
    "- Frequentist and Bayesian statistics \n",
    "- Functional analysis (transforms) and Calculus (derivatives)\n",
    "- Linear algebra, Linear programming and Matrix analysis\n",
    "- Optimization, Numerical methods, and Error Analysis\n",
    "- ... a lot of programming!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our agenda\n",
    "\n",
    "### ... so far\n",
    "- How we use **Numpy/Pandas library** for *reading* data\n",
    "- How we use **Matplotlib/Seaborn library** for *visualizing* data\n",
    "\n",
    "### onwards...\n",
    "- What is **linear regression**, and how does it work?\n",
    "- How do we **train** and **interpret** a linear regression model?\n",
    "- What are some **evaluation metrics** for regression problems?\n",
    "- How do we choose **which features to include** in my model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning taxonomy\n",
    "\n",
    "#### Supervised learning (*function aproximation*)\n",
    "- Classification and Regression\n",
    "    + goal: learn $f(x)$ from $\\{(x_i,y_i)\\}_{i=1}^{n}$\n",
    "    + target function: $f(x)$\n",
    "    + learned (estimated) function: $\\hat{f}(x)$\n",
    "    + training sample: $\\{(x_i,y_i)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning taxonomy\n",
    "\n",
    "#### Unsupervised learning (*data summarization*)\n",
    "- Clustering, **Dimentionality reduction**\n",
    "    + goal: learn $f(x)$ from $\\{x_i\\}_{i=1}^{n}$\n",
    "    + target function: $f(x)$\n",
    "    + learned (estimated) function: $\\hat{f}(x)$\n",
    "    + training sample: $\\{x_i\\}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning taxonomy\n",
    "\n",
    "#### Reinforcement learning (*scoring*)\n",
    "+ Learning + **Dynamic control**\n",
    "+ Environment-driven **behavior**\n",
    "+ Cummulative **reward maximization**\n",
    "    + utility theory, decision making, game theory\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning taxonomy\n",
    "\n",
    "We can formulate all of these different problems as some form of optimization\n",
    "\n",
    "+ Supervised learning\n",
    "+ Unsupervised learning\n",
    "+ Reinforcement learning\n",
    "\n",
    "> **Bib.Ref.** \n",
    "*Optimization by Vector Space Methods*, **David G. Luenberger**\n",
    "+ Department of Management Science and Engineering, Stanford University.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "# Calculus breviary\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Arithmetic fundamentals\n",
    "Do you remember Arithmetic?\n",
    "\n",
    "Following priority rules, get the results of the arithmetic operations below\n",
    "\n",
    "### $Q1$: $ 9 - 3 \\div \\frac{1}{3} + 1$ \n",
    "### $Q2$: $\\frac{ \\sqrt{5^{-2}\\cdot{\\sqrt[3]{5^{-3}\\cdot5^{-3}}} } }{5^{-2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "9 - (3 * (3**(-1))**(-1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "( (5**(-2)) * (5**(-3)*5**(-3))**(1/3) )**(1/2) / 5**(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus fundamentals\n",
    "**Calculus** is very different from arithmetic, algebra and geometry\n",
    "+ **calculus** is less static and more *dynamic*\n",
    "+ it is concerned with *change* and *motion* \n",
    "+ it deals with quantities that *approximate* other quantities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus fundamentals\n",
    "The primary building blocks or **abstract objects** that we deal with in calculus are **functions**. \n",
    "\n",
    "There are four possible ways to represent a function:\n",
    "+ verbally (description in words)\n",
    "+ numerically (by a table of values)\n",
    "+ visually (by a graph)\n",
    "+ **algebraically** (by explicit **formulas**)\n",
    "    + a concise way of expressing information **symbolically** about the **relationship** between given quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a function?\n",
    "\n",
    "We may think about functions in many different ways, \n",
    "but there are always three main parts:\n",
    "+ the **input**, the **relationship**, and the **output**\n",
    "\n",
    "An \"*intuitive explanation*\" of function would be as follows:\n",
    "> Def.1. \n",
    "A function is an **association rule** that *relates* **each element** of a **set** with **exactly one element** of another set \n",
    "(possibly the same set).\n",
    "1. \"*each element*\" means that every element in some set $X$ is related to some element in another set $Y$.\n",
    "    + some elements of $Y$ might not be related to at all, which is fine\n",
    "2. ***\"exactly one\"*** means that a function is **single-valued** (* emphatic term*)\n",
    "    + It will not give back ***two or more*** results for the same input \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a function?\n",
    "\n",
    "We can think of functions like **machines** that ***associate/map*** inputs to outputs\n",
    "> **NOTE**: The use of the term *machine* is due to historical reasons\n",
    "+ Evolution from fully-mechanical, to electro-mechanical to fully-electronic computers\n",
    "+ ACM (1947) Association for Computer Machinery\n",
    "\n",
    "![Machine learning](images/ml-functionmachine.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a function?\n",
    "\n",
    "This ***intuitive explanation*** of functions would be completed \n",
    "with a **formal definition**, which relies on the notion of the **Cartesian product of two sets** (*direct product of groups*). \n",
    "\n",
    "> Def. 2.\n",
    "The **Cartesian product** of two sets $X$ and $Y$ \n",
    "is the set of all **ordered pairs**, written $(x, y)$, \n",
    "where $x$ is an element of $X$ and $y$ is an element of $Y$. \n",
    "+ $x$ and the $y$ are called the **components** of the **ordered pair**. \n",
    "+ the ***Cartesian product*** of $X$ and $Y$ is denoted by $X\\times Y$.\n",
    "+ $X\\times Y  = \\{ \\;(x,y)\\; | \\; x\\in X, y\\in Y \\}$\n",
    "+ $G_1\\times G_2 = \\{ \\;(x,y)\\; | \\; x\\in G_1, y\\in G_2 \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a function?\n",
    "\n",
    "A function $f$ is commonly declared by stating its **domain** $X$ and **codomain** $Y$ using expressions like:\n",
    "\n",
    "+ $f: X\\to Y$    (e.g., $f:\\mathbb R^2\\to\\mathbb R$)\n",
    "+ $ X \\xrightarrow{f} Y$\n",
    "+ $ f\\{X\\}= Y$\n",
    "+ $f(x) = y$\n",
    "\n",
    "When written as $f(x)$, the elements $x\\in X$ are called arguments of $f$ \n",
    "+ for each argument $x\\in X$, the corresponding unique element in the codomain is called the **function value** at $x$, or the **image** of $x$ ***under*** $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When two functions are the same? \n",
    "Two functions are the same if they have the **same output for each input**\n",
    "+ Arithmetic method (tabular)\n",
    "+ Algebraic method (solve equations)\n",
    "\n",
    "> This is a fundamental idea for **function approximation**\n",
    "\n",
    "Many problems can be solved *arithmetically* or *algebraically*. \n",
    "+ An algebraic solution is simply a *generalized arithmetic solution*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When two functions are the same? \n",
    "\n",
    "#### $Q$: Is $f(x) = (1+x)^2$ the same as $g(x) = x^2 + 2x +1$ ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When two functions are the same? \n",
    "\n",
    "#### $Q$: Is $f(x) = (1+x)^2$ the same as $g(x) = x^2 + 2x +1$ ?\n",
    ">  ***Algebraically...***\n",
    "$$\n",
    "\\begin{align*}\n",
    " f(x) &= (1+x)^2 \\\\\n",
    "      &= (1+x)(1+x) \\\\\n",
    "      &= 1*(1+x)+ x*(1+x) \\\\ \n",
    "      &= 1*1 + 1*x + x*1 + x*x \\\\ \n",
    "      &= 1^2 + x + x + x^2 \\\\\n",
    "      &= 1 + 2x + x^2 \\\\\n",
    "      &= g(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> ***Arithmetically...***\n",
    "\n",
    "input value | f(x) | g(x)\n",
    "---|---|---\n",
    "-1 | 0 | 0\n",
    " 0 | 1 | 1\n",
    " 1 | 4 | 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When two functions are the same? \n",
    "\n",
    "#### $Q$: Is $f(x)=\\frac{x^2}{x}$ the same as $g(x) = x $ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When two functions are the same? \n",
    "\n",
    "#### $Q$: Is $f(x)=\\frac{x^2}{x}$ the same as $g(x) = x $ ?\n",
    ">  ***Algebraically...***\n",
    "$$\n",
    "\\begin{align*}\n",
    " f(x) &= \\frac{x^2}{x}\\\\\n",
    "      &= (x^2)(x^{-1}) \\\\\n",
    "      &= (x^{2-1}) \\\\ \n",
    "      &= (x^{1}) \\\\ \n",
    "      &= x \\\\\n",
    "      &= g(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    ">  ***Arithmetically...***\n",
    "\n",
    "input value | f(x) | g(x)\n",
    "---|---|---\n",
    " 2 | 2 | 2\n",
    "-2 | -2 | -2\n",
    " 0 | ? | 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "# REGRESSION ANALYSIS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Analysis\n",
    "1. What is Regression Analysis?\n",
    "2. Why do we use Regression Analysis?\n",
    "3. What are the types of Regressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Regression Analysis?\n",
    "**Regression analysis** is a **statistical process** for investigating the **functional relationships** among variables.\n",
    "\n",
    "**Regression analysis** is a form of **modelling technique** which estimates the **conditional expectation** of ***dependent variables*** given the ***independent variables***.\n",
    "+ the *average value* of the *dependent variable* when the *independent variables* are *fixed*\n",
    "\n",
    "$$ \\mathbf{E}[Y\\;|\\; \\mathbf{X}]$$\n",
    "\n",
    "The **estimation target** is a **function** of the independent variables called the **regression function**\n",
    "+ It is highly related to ***fitting curves*** to a given set of points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistics\n",
    "\n",
    "Statistics is that *branch of mathematics* that analyzes and presents **inferences** from  **data** asssociated to **samples** drawn from the population, instead of the **entire population**. \n",
    "\n",
    "Concepts to understand population's **moments of distribution**:\n",
    "+ Central tendency, dispersion, shape\n",
    "+ Mean (*first moment*),  value around which central clustering occurs\n",
    "+ Variance (*second moment*), degree of variability around the mean\n",
    "+ Skewness (*third moment*), degree of asymmetry of a distribution around its mean\n",
    "+ Kurtosis (*fourth moment*), measure of the *tailedness* of the distribution \n",
    "![Normal distribution](images/normal_pdf.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why  use Regression Analysis?\n",
    "\n",
    "Is an important tool for **data analysis** and finding the *cause-effect relationship* between variables \n",
    "+ this technique is used for forecasting, time series modelling \n",
    "\n",
    "There are multiple benefits of using regression analysis. \n",
    "+ It allows to get insights about **significant relationships** between dependent and independent variables\n",
    "+ It is **highly interpretable** and is basis for other methods\n",
    "+ Allows us to **compare** the effects of variables measured on **different scales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are the types of Regressions?\n",
    "\n",
    "There are various types of regression techniques available to make predictions. \n",
    "+ Linear, Logistic, Polynomial, Stepwise, Ridge, and Lasso\n",
    "\n",
    "These techniques are mostly driven by three metrics \n",
    "1. number of independent variables\n",
    "2. type of dependent variables \n",
    "3. shape of regression line (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "# LINEAR REGRESSION ANALYSIS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Analysis\n",
    "\n",
    "It is one of the most widely known technique, and is usually among the first topics people pick while learning ***predictive modeling***. \n",
    "\n",
    "> Nowadays, due to the flood of **big data** emanating from **businesses processes**, companies are adopting **analytic solutions** to extract **meaningfrom data** and to improve **decision making**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Analysis\n",
    "\n",
    "**Business intelligence & analytics** focuses on optimizing CAPEX/OPEX, by applying **statistical and analytical methods** on historical **data**, to understand **business performance**. \n",
    "\n",
    "> Analytical options are *categorized* into three distinct but complementary types: \n",
    "+ **Descriptive**, which answer ***What has happened?*** by using data aggregation and data mining to provide *insight into the past* \n",
    "+ **Predictive**, which answer ***What could happen?*** by using statistical and forecasting models to *understand the future*\n",
    "+ **Prescriptive**, which answer ***What should we do?*** by using optimization and simulation algorithms to *advice on possible outcomes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Analysis\n",
    "\n",
    "**Coursera Specializations**:\n",
    "+ Strategic Business Analytics Specialization (4 courses)\n",
    "+ Business Analytics Specialization (5 courses)\n",
    "+ Advanced Business Analytics Specialization \n",
    "+ Excel to MySQL: Analytic Techniques for Business Specialization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Analysis\n",
    "\n",
    "In this technique, the **nature of regression is linear**, i.e., it suppose a ***linear relationship*** between features and response\n",
    "+ **Pros:** fast, highly interpretable, well-understood\n",
    "+ **Cons:** unlikely to produce the best **predictive accuracy** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Analysis\n",
    "\n",
    "\n",
    "**Accuracy-Precision** trade-off (a.k.a. *bias-variance trade-off*) \n",
    "+ **Accuracy** is how close a measured/predicted value is to the true value (*constant parameter*).\n",
    "+ **Precision** is how close the measured/predicted values are to each other. \n",
    "![alt text](./images/accuracy_precision.svg \"as\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "In this technique, the **dependent variable** is **continuous**,\n",
    "**independent variable**(s) can be **continuous or discrete** \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "y &= f(\\mathbf{x}) \\\\\n",
    "  &= \\beta_0 + \\beta_{1}x_{1} + \\ldots + \\beta_{k}x_{k} + \\ldots + \\beta_{n}x_{n} \\\\\n",
    "  &= \\sum_{i}^n \\beta_{i}x_{i} \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "What does each term represent?\n",
    "\n",
    "- $y$ is the response\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the $1$st feature)\n",
    "- $\\beta_k$ is the coefficient for $x_k$ (the $k$th feature)\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the $n$th feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "y &= \\beta_0 + \\beta_{1}x_{1} + \\ldots + \\beta_{k}x_{k} + \\ldots + \\beta_{n}x_{n} \\\\\n",
    "  &= \\sum_{i}^n \\beta_{i}x_{i} \\\\\n",
    "  &= \\langle  \\boldsymbol{\\beta} , \\mathbf{x} \\rangle \\\\\n",
    "  &= \\boldsymbol{\\beta}^{\\top}\\mathbf{x}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "What does each term represent?\n",
    "\n",
    "- $y$ is the response value\n",
    "- $\\mathbf{x}$ is the feature vector\n",
    "- $\\boldsymbol{\\beta}$ is the vector of coefficients\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "y &= f(\\mathbf{x}) \\\\\n",
    "  &= \\beta_0 + \\beta_{1}x_{1} + \\ldots + \\beta_{k}x_{k} + \\ldots + \\beta_{n}x_{n} \\\\\n",
    "  &= \\sum_{i}^n \\beta_{i}x_{i} \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "> **NOTE**: \n",
    "The $\\beta$ values are called the **model coefficients** \n",
    "+ These values are ***learned*** during the ***model fitting*** step using some **optimization criterion**. \n",
    "+ Then, the **fitted model** can be used to make **predictions**!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "# Optimization criterion: error terms\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "Simple linear regression is an approach for predicting a **quantitative response** using a **single feature** (predictor or input variable). It takes the following form:\n",
    "\n",
    "$y = f(x) = \\beta_0 + \\beta_1x$\n",
    "\n",
    "### What does each term represent?\n",
    "\n",
    "- $y$ is the response (variable we are trying to predict)\n",
    "- $x$ is the feature (variable we are using to predict)\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Model Coefficients\n",
    "\n",
    "How do the **model coefficients** *relate* to the some **optimization criterion**?\n",
    "\n",
    "![Slope-intercept](images/slope_intercept_knob.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning  Model Coefficients\n",
    "\n",
    "Generally, coefficients are ***learned*** (estimated) using the **least squares criterion**, which means we are looking for a **linear model** (a line),  which ***minimizes*** the **sum of squared residuals** (or *sum of squared errors*):\n",
    "![Estimating coefficients](images/estimating_coefficients.png)\n",
    "\n",
    "> **NOTE**: \n",
    "- The black dots are the **observed values** (given $x$ and $y$).\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **errors**, i.e., the *distances* between the observed values and the least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Error terms\n",
    "\n",
    "In a statistical or mathematical model, an **error term** is a **random variable**, which is created when the model does ***not fully represent*** the actual **relationship** between the independent and dependent variables. \n",
    "+ Essentially means that the model is **not completely accurate**\n",
    "\n",
    "As a result of this ***incomplete relationship***, the error term \n",
    "is a **quantitative measure** of how much our linear model $\\hat{f}$\n",
    "differs from the actual functional relationship $f$. \n",
    "\n",
    "> Error term represents the **margin of error**, referring to the **sum of the deviations** within the regression line\n",
    "+ the **error term** is also known as the **residual** or **disturbance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model evaluation metrics for regression\n",
    "\n",
    "We need **evaluation metrics** designed for comparing continuous values. \n",
    "\n",
    "Let's create some example numeric predictions, and calculate \n",
    "**three common evaluation metrics** for regression problems:\n",
    "\n",
    "- **Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:\n",
    "$$ \\frac{1}{n}\\sum_{i=1}^n|y_i-\\hat{y}_i| $$\n",
    "\n",
    "- **Mean Squared Error (MSE)**  \"punishes\" larger errors, taking just the mean of the squared errors:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)** is interpretable in the \"y\" units, since it is the square root of the mean of the squared errors:\n",
    "$$\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# calculate Mean Absolute Error (MAE) \n",
    "y = np.array([100, 50, 30, 20])\n",
    "y_hat = np.array([90, 50, 50, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{n}\\sum_{i=1}^n|y_i-\\hat{y}_i|  &= \\frac{1}{n}(|y_1-\\hat{y}_1| + \\ldots + |y_n-\\hat{y}_n|)  \\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by hand\n",
    "(10 + 0 + 20 + 10) #/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( abs(y[0]-y_hat[0]) +\n",
    "  abs(y[1]-y_hat[1]) +\n",
    "  abs(y[2]-y_hat[2]) + \n",
    "  abs(y[3]-y_hat[3]) ) #/4.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 0, 20, 10]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(y) == len(y_hat)\n",
    "[ abs(y[i]-y_hat[i])  for  i  in  range(0,len(y)) ]  # list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 90), (50, 50), (30, 50), (20, 30)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( zip(y,y_hat) )  # 'tuple' does not support item assignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 0, 20, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ abs(a[0]-a[1])  for  a  in  zip(y,y_hat)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 0, 20, 10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( map( lambda a:abs(a[0]-a[1]), zip(y,y_hat) ) ) # scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 0, 20, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_value = np.array([100, 50, 30, 20])\n",
    "pred_value = np.array([90, 50, 50, 30])\n",
    "\n",
    "niter = iter(zip(true_value,pred_value))\n",
    "\n",
    "items = np.array([i for i in niter]) # [0][0]\n",
    "\n",
    "[ np.abs(item[0]-item[1]) for item in items ]\n",
    "# np.sum(\n",
    "# [ np.abs(item[0]-item[1]) for item in items] \n",
    "# ) * (1/len(items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 0, 20, 10]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "true_value = np.array([100, 50, 30, 20])\n",
    "pred_value = np.array([90, 50, 50, 30])\n",
    "index = range(1, len(true_value)+1)\n",
    "\n",
    "niter = iter(zip(true_value, pred_value))\n",
    "items = pd.Series(data=[i for i in niter], index= index) # .iloc[0][0] #[0][0]\n",
    "\n",
    "[ pd.Series.abs(item[0]-item[1])  for  item  in  items ]\n",
    "#pd.Series(\n",
    "#  [pd.Series.abs(item[0]-item[1]) for item in items]\n",
    "# ).sum() * (1/len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.0\n"
     ]
    }
   ],
   "source": [
    "# calculate MSE by hand\n",
    "print((10**2 + 0**2 + 20**2 + 10**2)/4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.24744871391589\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE by hand\n",
    "#print(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\n",
    "print(((10**2 + 0**2 + 20**2 + 10**2)/4.)**(1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSE vs MSE:  fitting vs evaluation\n",
    "\n",
    "When we want to **build a model** (linear regression in our case), \n",
    "we would usually use the **Least Square Error (LSE)** method \n",
    "+ **minimizing the total euclidean distance** between a line and the data points \n",
    "> Theoretically the coefficients of this line can be found using calculus but in practice, an algorithm will perform a **gradient descent** which is faster.\n",
    "\n",
    "Once we have your model, we want to **evaluate its performance**; \n",
    "thus, in the case of regression, the **Mean Squared Error (MSE)** \n",
    "is good to evaluate ***how far  in average*** is our model \n",
    "from the actual data points (or test dataset). \n",
    "\n",
    "> **NOTE**: **LSE** is a method that **builds a model**, and **MSE** is a metric that evaluate **model's performance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "# Linear Regression: Example\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's suppose we are given the followign dataset:\n",
    "\n",
    "point | x | y\n",
    "- | - | -\n",
    "$p_1$ | 1 | 1\n",
    "$p_2$ | 3 | 2\n",
    "\n",
    "> In this example, we apply linear regression analysis to this sophisticated dataset, and we show how to interpret the results of our analysis. \n",
    "\n",
    "> We will do the computations manually, since the details are very valuable, in an educational sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHHxJREFUeJzt3XuUXWWZ5/HvLxdu0qsBE+2qQEi6zVqKV6pqIj122zSO\ndHRVER1dq+MVXHaVbRu1ey6KM90UFZ0ZtGf1xREbz8G00q2gA+LUqQWNrEYHRxs7dY4IJIikUYbU\nqTVEIggSLkme+ePswKGspHaS2req32ets3L23u8+7/NyeM9T+333RRGBmZnZXJYUHYCZmVWDE4aZ\nmaXihGFmZqk4YZiZWSpOGGZmlooThpmZpeKEYWZmqThhmJlZKk4YZmaWyrKiA5hPK1asiDVr1hQd\nhplZZTSbzZ9GxMo0ZRdUwlizZg2Tk5NFh2FmVhmS7k9b1kNSZmaWihOGmVmFTT28lzdd/h2mHt6b\neV1OGGZmFXbp+Hbu2PUwl45vz7wuJwwzs4pq3r+Hb9+7mwMB3753N83792RanxOGmVkFHTgQfPS6\nO3ni6QMAPPH0AS6+7k4OHMjuGUeZJQxJZ0j6pqQdkrZL+vAsZSTp05J2SrpDUl/Xtgsl3Zu8Lswq\nTjOzKrr++1O0Z8xbTD28l6/fPpVZnVkeYewD/n1EnAWcA3xA0lkzyrwBWJe8RoC/AZB0GjAKvBpY\nD4xKOjXDWM3MKuMXT+5jy8QOHn9q/3PWP/7UfrY0dvD4U/syqTez6zAiYhqYTt4/KuluYBWwo6vY\nRuCq6Dwn9jZJp0jqAc4Fbo6IPQCSbgY2AFdnFe/Y2BgAw8PD9Pb20mg0aLVa9PX1MTQ0RLvdpl6v\nAzA6OgpArVZjenqawcFB+vv7aTabTExM0NPTw8jIiD/Xn+vP9edm8rmN+5ewlx5m88S+/Xzmlp18\nZMOLZ91+LHKZw5C0Bjgb+N6MTauAB7qWdyXrDrV+ts8ekTQpaXL37t3zFbKZWWnds38lMw4unvHE\n0wf4+9tSX4t3RNT54z47kk4G/jfwXyLiazO2TQCXRcT/SZb/EfgonSOMEyLiE8n6PwP2RsR/P1xd\nAwMD4Su9zWyh++SNP+Rvv/vjZya8u524fAnvec3a1EcYkpoRMZCmbKZHGJKWA9cBX5qZLBJTwBld\ny6cn6w613sxs0dt83os4ftnSWbcdv2wpm897USb1ZnmWlIDPA3dHxF8cotg48O7kbKlzgEeSuY+b\ngPMlnZpMdp+frDMzW/Sed/wyLhk8i5OOe27SOOm4pVwydBYnHZfN9HSWRxivAd4FnCfp9uT1Rkl/\nKOkPkzI3APcBO4E68EcAyWT3x4FtyWvLwQlwMzODN5+9it5TTnzOulWnnMibXjXrdO+8yHwOI0+e\nwzCzxaR5/x7eceX3eOLpA5ywfAlf+oNz6D/zyK5AKM0chpmZZaf/zNN4xcrlCPjtdSuPOFkcKScM\nM7MKO/OhbazQY1x6wUszr2tBPUDJzGyxWbfq+azjEVbNmM/IghOGmVmFHbxqPA8ekjIzs1ScMMzM\nKmxsbOyZe1JlzQnDzMxS8RyGmVmFDQ8P51aXE4aZWYX19vbmVpeHpMzMKqzRaNBoNHKpywnDzKzC\nWq0WrVYrl7o8JGVmVmF9fX251eWEYWZWYUNDQ7nV5SEpM7MKa7fbtNvtXOpywjAzq7B6vU69Xs+l\nrsyGpCRtBQaBByPiZbNs/4/AO7rieAmwMiL2SPoJ8CiwH9iX9l7tZmaWncweoCTptcBjwFWzJYwZ\nZYeAP4mI85LlnwADEfHTI6nTD1AyMzsypXiAUkTcCqR9rOrbgKuzisXMzI5d4XMYkk4CNgDXda0O\n4BuSmpLyu3evmVnF1Go1arVaLnWV4bTaIeA7EdF9NPJbETEl6QXAzZJ+mByx/JIkoYwArF69Ovto\nzcxKZHp6Ore6ypAwNjFjOCoippJ/H5R0PbAemDVhREQNqEFnDiPbUM3MymVwcDC3ugpNGJJ+Ffgd\n4J1d654HLImIR5P35wNbCgrRzKzU+vv7c6sry9NqrwbOBVZI2gWMAssBIuKKpNibgW9ExC+6dn0h\ncL2kg/F9OSL+Ias4zcyqrNlsAvkkjswSRkS8LUWZLwBfmLHuPuCV2URlZrawTExMABVPGGZmlr2e\nnp7c6nLCMDOrsJGR/K48KPw6DDMzqwYnDDOzChsbG2NsbCyXupwwzMwsFc9hmJlV2PDwcG51OWGY\nmVVYb29vbnV5SMrMrMIajQaNRiOXupwwzMwqrNVq0Wq1cqnLQ1JmZhXW19eXW11OGGZmFTY0NJRb\nXR6SMjOrsHa7TbvdzqUuJwwzswqr1+vU6/Vc6nLCMDOzVDyHYWZWYaOjo7nV5SMMMzNLJbOEIWmr\npAcl3XWI7edKekTS7cnrkq5tGyTdI2mnpIuzitHMrOpqtRq1Wi2XurIckvoC8BngqsOU+XZEPOcJ\n5pKWApcDrwd2AdskjUfEjqwCNTOrqunp6dzqyvIRrbdKWnMUu64HdiaPakXSNcBGwAnDzGyGwcHB\nuQvNk6InvX9T0g+ANvAfImI7sAp4oKvMLuDVRQRnZlZ2eTzL+6AiE0YLODMiHpP0RuDrwLoj/RBJ\nI8AIwOrVq+c3QjOzkms2m0A+iaOws6Qi4ucR8Vjy/gZguaQVwBRwRlfR05N1h/qcWkQMRMTAypUr\nM43ZzKxsJiYmmJiYyKWuwo4wJP0a8P8iIiStp5O8HgIeBtZJWksnUWwC3l5UnGZmZdbT05NbXZkl\nDElXA+cCKyTtAkaB5QARcQXwVuD9kvYBe4FNERHAPkmbgZuApcDWZG7DzMxmGBkZya0udX6jF4aB\ngYGYnJwsOgwzs8qQ1IyIgTRlfaW3mVmFjY2NMTY2lktdThhmZpZK0ddhmJnZMRgeHs6tLicMM7MK\n6+3tza0uD0mZmVVYo9Gg0WjkUpcThplZhbVaLVqtVi51eUjKzKzC+vr6cqvLCcPMrMKGhoZyq8tD\nUmZmFdZut2m327nU5YRhZlZh9Xqder2eS11OGGZmlornMMzMKmx0dDS3unyEYWZmqThhmJlVWK1W\no1ar5VKXh6TMzCpseno6t7qcMMzMKmxwcDC3urJ84t5WYBB4MCJeNsv2dwAfBQQ8Crw/In6QbPtJ\nsm4/sC/twz3MzBab/v7+3OrKcg7jC8CGw2z/MfA7EfFy4OPAzEG4342IVzlZmJkdWrPZpNls5lJX\nZkcYEXGrpDWH2f7drsXbgNOzisXMbKGamJgA8jnSKMscxnuBG7uWA/iGpAA+FxGHPAVA0ggwArB6\n9epMgzQzK5uenp7c6lJEZPfhnSOMidnmMLrK/C7wWeC3IuKhZN2qiJiS9ALgZuCDEXHrXPUNDAzE\n5OTkvMRuZrYYSGqmHfov9DoMSa8ArgQ2HkwWABExlfz7IHA9sL6YCM3M7KDCEoak1cDXgHdFxI+6\n1j9P0q8cfA+cD9xVTJRmZuU2NjbG2NhYLnVleVrt1cC5wApJu4BRYDlARFwBXAI8H/isJHj29NkX\nAtcn65YBX46If8gqTjMzSyfTOYy8eQ7DzBabg8/C6O3tPar9j2QOoyxnSZmZ2VE42kRxNHzzQTOz\nCms0GjQajVzqcsIwM6uwVqtFq9XKpS4PSZmZVVhfX19udTlhmJlV2NDQUG51eUjKzKzC2u32M2dK\nZc0Jw8yswur1OvV6PZe6nDDMzCwVz2GYmVXY6OhobnX5CMPMzFJxwjAzq7BarUatdshHBs0rD0mZ\nmVXY9PR0bnX5CMNKaerhvbzp8u8w9fDeokMxK7XBwUEGBwdzqcsJw0rp0vHt3LHrYS4d3150KGal\n1t/fn8vzvMEJw0qoef8evn3vbg4EfPve3TTv31N0SGal1Ww2aTabudSVacKQtFXSg5JmfWKeOj4t\naaekOyT1dW27UNK9yevCLOO08jhwIPjodXfyxNMHAHji6QNcfN2dHDiwcJ7bYjafJiYmmJiYyKWu\nrI8wvgBsOMz2NwDrktcI8DcAkk6j84S+V9N5nveopFMzjdRK4frvT9GeMW8x9fBevn77VEERmZVb\nT08PPT09udQ1Z8KQ9MGj/bGOiFuBw40nbASuio7bgFMk9QC/B9wcEXsi4mfAzRw+8dgC8Isn97Fl\nYgePP7X/Oesff2o/Wxo7ePypfQVFZlZeIyMjjIyM5FJXmtNqXwhsk9QCtgI3xfw913UV8EDX8q5k\n3aHWZ+bgQ9SHh4fp7e2l0WjQarXo6+tjaGiIdrv9zP1aDl5ZWavVmJ6eZnBwkP7+fprNJhMTE/T0\n9DzzBfpz03/u+y6/gcf2vwBY+kvfzxP79vOZW3bykQ0vPsZv2syO1pxHGBHxp3SGjD4PXATcK+m/\nSvqNjGNLRdKIpElJk7t37y46HDsG9+xfyf5ZkgV05jL+/rb7c47IrPzGxsae+YMsa0p7sCDplcB7\n6AwNfRM4h86w0Ufm2G8NMBERL5tl2+eAb0XE1cnyPcC5B18R8b7Zyh3KwMBATE5OpmqPlc8nb/wh\nf/vdHz8z4d3txOVLeM9r1voIw2yGg8niaO8pJakZEQNpyqaZw/iwpCbwKeA7wMsj4v1AP/CWo4rw\nWePAu5Ozpc4BHomIaeAm4HxJpybzJ+cn62wB23zeizh+2exHGMcvW8rm816Uc0Rm5Tc8PMzw8HAu\ndaWZwzgN+LcR8ZzxgIg4IOmwlxdKuprO0cIKSbvonPm0PNn/CuAG4I3ATuBxOkcwRMQeSR8HtiUf\ntSUifDL+Ave845dxyeBZ/Nn/uus5E98nHbeUS4bO4qTjfCcbs5l6e3tzqyv1kFQVeEiq+g4cCM7/\nq1vZ+eCjgABY94KTuemPX8uSJSo2OLMSajQawNE/qnVeh6TM8rRkifjkW17OUjrzGCcsX8Jlb3mF\nk4XZIbRaLVqtVi51+RjfSqf/zNM467Ql3LUn+O11K+k/09dsmh1KX1/f3IXmiROGldIVI6/jA19q\ncekFLy06FLNSO9qhqKPhhGGlpMd/xmffvJbeU04sOhSzUmu320A+k9+ew7BSqtfrz1xRbmaHlmdf\nccIwM7NUfFqtmdki5tNqzcxs3jlhWCnVajVqtVrRYZiVXp59xWdJWSlNT08XHYJZJeTZV5wwrJQG\nBw97mzIzS+TZV5wwrJT6+/uLDsGsEvLsK57DsFJqNps0m82iwzArvTz7io8wrJQmJiYAH2mYzSXP\nvuKEYaXU09NTdAhmlZBnX/GFe2Zmi1hpLtyTtEHSPZJ2Srp4lu1/Ken25PUjSQ93bdvftW08yzjN\nzGxumQ1JSVoKXA68HtgFbJM0HhE7DpaJiD/pKv9B4Oyuj9gbEa/KKj4rt2N9sL3ZYpFnX8nyCGM9\nsDMi7ouIp4BrgI2HKf824OoM4zEzs2OQ5aT3KuCBruVdwKtnKyjpTGAtcEvX6hMkTQL7gMsi4utZ\nBWrlMzw8XHQIZpWQZ18py1lSm4BrI2J/17ozI2JK0q8Dt0i6MyL+ZeaOkkaAEYDVq1fnE61lLo+H\nwZgtBHn2lSyHpKaAM7qWT0/WzWYTM4ajImIq+fc+4Fs8d36ju1wtIgYiYmDlypXHGrOVRKPRoNFo\nFB2GWenl2VeyTBjbgHWS1ko6jk5S+KWznSS9GDgV+KeudadKOj55vwJ4DbBj5r62cLVaLVqtVtFh\nmJVenn0lsyGpiNgnaTNwE7AU2BoR2yVtASYj4mDy2ARcE8+9IOQlwOckHaCT1C7rPrvKFr6+vr6i\nQzCrhDz7ii/cMzNbxEpz4Z7Z0Wq327Tb7aLDMCu9PPuKE4aVUr1ep16vFx2GWenl2VecMMzMLBXP\nYZiZLWKewzAzs3nnhGGlVKvVqNVqRYdhVnp59pWy3BrE7Dmmp6eLDsGsEvLsK04YVkqDg4NFh2BW\nCXn2FScMKyU/y9ssnTz7iucwrJSazSbNZrPoMMxKL8++4iMMK6WJiQnARxpmc8mzrzhhWCn19PQU\nHYJZJeTZV3zhnpnZIuYL98zMbN45YVgpjY2NMTY2VnQYZqWXZ19xwjAzs1QynfSWtAH4azpP3Lsy\nIi6bsf0i4M959lnfn4mIK5NtFwJ/mqz/RER8MctYrVyGh4eLDsGsEvLsK5klDElLgcuB1wO7gG2S\nxmd51OpXImLzjH1PA0aBASCAZrLvz7KK18qlt7e36BDMKiHPvpLlkNR6YGdE3BcRTwHXABtT7vt7\nwM0RsSdJEjcDGzKK00qo0WjQaDSKDsOs9PLsK1kmjFXAA13Lu5J1M71F0h2SrpV0xhHui6QRSZOS\nJnfv3j0fcVsJtFotWq1W0WGYlV6efaXoC/cawNUR8aSk9wFfBM47kg+IiBpQg851GPMfohWhr6+v\n6BDMKiHPvpJlwpgCzuhaPp1nJ7cBiIiHuhavBD7Vte+5M/b91rxHaKU1NDRUdAhmlZBnX8lySGob\nsE7SWknHAZuA8e4Ckrqvab8AuDt5fxNwvqRTJZ0KnJ+ss0Wi3W7TbreLDsOs9PLsK5kljIjYB2ym\n80N/N/DViNguaYukC5JiH5K0XdIPgA8BFyX77gE+TifpbAO2JOtskajX69Tr9aLDMCu9PPtKpnMY\nEXEDcMOMdZd0vf8Y8LFD7LsV2JplfGZmlp5vPmhmtoj55oNmZjbvnDCslGq1GrVaregwzEovz75S\n9HUYZrOanp4uOgSzSsizrzhhWCkNDg4WHYJZJeTZV5wwrJT8LG+zdPLsK57DsFJqNps0m82iwzAr\nvTz7io8wrJQmJiYAH2mYzSXPvuKEYaXU09MzdyEzy7Wv+MI9M7NFzBfumZnZvHPCsFIaGxtjbGys\n6DDMSi/PvuKEYWZmqXjS20ppeHi46BDMKiHPvuKEYaXU29tbdAhmlZBnX/GQlJVSo9Gg0WgUHYZZ\n6eXZVzJNGJI2SLpH0k5JF8+y/d9J2iHpDkn/KOnMrm37Jd2evMZn7msLW6vVotVqFR2GWenl2Vcy\nG5KStBS4HHg9sAvYJmk8InZ0Ffs+MBARj0t6P/Ap4PeTbXsj4lVZxWfl1tfXV3QIZpWQZ1/Jcg5j\nPbAzIu4DkHQNsBF4JmFExDe7yt8GvDPDeKxChoaGig7BrBLy7CtZDkmtAh7oWt6VrDuU9wI3di2f\nIGlS0m2S3nSonSSNJOUmd+/efWwRW2m0223a7XbRYZiVXp59pRST3pLeCQwAf961+szkcvW3A38l\n6Tdm2zciahExEBEDK1euzCFay0O9Xqderxcdhlnp5dlXskwYU8AZXcunJ+ueQ9K/Af4zcEFEPHlw\nfURMJf/eB3wLODvDWM3MbA6Z3XxQ0jLgR8Dr6CSKbcDbI2J7V5mzgWuBDRFxb9f6U4HHI+JJSSuA\nfwI2zpgw/yW++aCZ2ZE5kpsPZjbpHRH7JG0GbgKWAlsjYrukLcBkRIzTGYI6GfifkgD+b0RcALwE\n+JykA3SOgi6bK1mYmVm2fHtzK6VarQbAyMhIwZGYldux9pVSHGGYHYvp6emiQzCrhDz7ihOGldLg\n4GDRIZhVQp59xQnDSsnP8jZLJ8++UorrMMxmajabNJvNosMwK708+4qPMKyUJiYmAB9pmM0lz77i\nhGGl1NPTU3QIZpWQZ1/xabVmZovYkZxW6zkMMzNLxQnDSmlsbIyxsbGiwzArvTz7ihOGmZml4klv\nK6Xh4eGiQzCrhDz7ihOGlVJvb2/RIZhVQp59xUNSVkqNRoNGo1F0GGall2dfccKwUmq1WrRaraLD\nMCu9PPuKh6SslPr6+ooOwawS8uwrmV64J2kD8Nd0HqB0ZURcNmP78cBVQD/wEPD7EfGTZNvHgPcC\n+4EPRcRNc9XnC/fMzI5MKS7ck7QUuBx4A3AW8DZJZ80o9l7gZxHxIuAvgU8m+54FbAJeCmwAPpt8\nni0S7XabdrtddBhmpZdnX8lyDmM9sDMi7ouIp4BrgI0zymwEvpi8vxZ4nTrPat0IXBMRT0bEj4Gd\nyefZIlGv16nX60WHYVZ6efaVLBPGKuCBruVdybpZy0TEPuAR4Pkp9zUzsxxVftJb0ggwArB69eqC\no7H5Mjo6WnQIZpWQZ1/J8ghjCjija/n0ZN2sZSQtA36VzuR3mn0BiIhaRAxExMDKlSvnKXQzM5sp\ny4SxDVgnaa2k4+hMYo/PKDMOXJi8fytwS3RO2xoHNkk6XtJaYB3wzxnGamZmc8hsSCoi9knaDNxE\n57TarRGxXdIWYDIixoHPA38naSewh05SISn3VWAHsA/4QETszypWMzObmx+gZGa2iJXiOgwzM1tY\nnDDMzCwVJwwzM0vFCcPMzFJxwjAzs1QW1FlSknYD9x/l7iuAn85jOEVaKG1ZKO0At6WMFko74Nja\ncmZEpLrqeUEljGMhaTLtqWVlt1DaslDaAW5LGS2UdkB+bfGQlJmZpeKEYWZmqThhPKtWdADzaKG0\nZaG0A9yWMloo7YCc2uI5DDMzS8VHGGZmlsqiSxiSNki6R9JOSRfPsv14SV9Jtn9P0pr8o5xbinZc\nJGm3pNuT1x8UEedcJG2V9KCkuw6xXZI+nbTzDkl9eceYVoq2nCvpka7v5JK8Y0xL0hmSvilph6Tt\nkj48S5nSfzcp21GJ70XSCZL+WdIPkraMzVIm29+viFg0Lzq3Wf8X4NeB44AfAGfNKPNHwBXJ+03A\nV4qO+yjbcRHwmaJjTdGW1wJ9wF2H2P5G4EZAwDnA94qO+Rjaci4wUXScKdvSA/Ql738F+NEs/4+V\n/rtJ2Y5KfC/Jf+eTk/fLge8B58wok+nv12I7wlgP7IyI+yLiKeAaYOOMMhuBLybvrwVeJ0k5xphG\nmnZUQkTcSudZKIeyEbgqOm4DTpHUk090RyZFWyojIqYjopW8fxS4G1g1o1jpv5uU7aiE5L/zY8ni\n8uQ1cxI609+vxZYwVgEPdC3v4pf/53mmTETsAx4Bnp9LdOmlaQfAW5KhgmslnTHL9ipI29aq+M1k\nSOFGSS8tOpg0kmGNs+n8RdutUt/NYdoBFfleJC2VdDvwIHBzRBzyO8ni92uxJYzFpAGsiYhXADfz\n7F8dVpwWndswvBL4H8DXC45nTpJOBq4D/jgifl50PEdrjnZU5nuJiP0R8SrgdGC9pJflWf9iSxhT\nQPdf2qcn62YtI2kZ8KvAQ7lEl96c7YiIhyLiyWTxSqA/p9jmW5rvrBIi4ucHhxQi4gZguaQVBYd1\nSJKW0/mR/VJEfG2WIpX4buZqR9W+F4CIeBj4JrBhxqZMf78WW8LYBqyTtFbScXQmhcZnlBkHLkze\nvxW4JZIZpBKZsx0zxpIvoDN2W0XjwLuTM3LOAR6JiOmigzoakn7t4HiypPV0+l/Z/hgBOmdAAZ8H\n7o6IvzhEsdJ/N2naUZXvRdJKSack708EXg/8cEaxTH+/ls3XB1VBROyTtBm4ic6ZRlsjYrukLcBk\nRIzT+Z/r7yTtpDOBuam4iGeXsh0fknQBsI9OOy4qLODDkHQ1nbNUVkjaBYzSmcwjIq4AbqBzNs5O\n4HHgPcVEOrcUbXkr8H5J+4C9wKYS/jFy0GuAdwF3JmPmAP8JWA2V+m7StKMq30sP8EVJS+kkta9G\nxESev1++0tvMzFJZbENSZmZ2lJwwzMwsFScMMzNLxQnDzMxSccIwM7NUnDDMzCwVJwwzM0vFCcMs\nI5L+VXLzxxMkPS95hkGu9/4xm0++cM8sQ5I+AZwAnAjsioj/VnBIZkfNCcMsQ8m9vrYBTwD/OiL2\nFxyS2VHzkJRZtp4PnEznaW8nFByL2THxEYZZhiSN03ki4lqgJyI2FxyS2VFbVHerNcuTpHcDT0fE\nl5M7jH5X0nkRcUvRsZkdDR9hmJlZKp7DMDOzVJwwzMwsFScMMzNLxQnDzMxSccIwM7NUnDDMzCwV\nJwwzM0vFCcPMzFL5/13bwo9xOs87AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b46e574a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# (1,1), (3,2)\n",
    "plt.plot([0, 1], [1,1], color='grey', linewidth=2,linestyle='dotted')\n",
    "plt.plot([1,1],[0,1], color='grey', linewidth=2,linestyle='dotted')\n",
    "plt.plot([0, 3], [2,2], color='grey', linewidth=2,linestyle='dotted')\n",
    "plt.plot([3,3],[0,2], color='grey', linewidth=2,linestyle='dotted')\n",
    "plt.plot([1,3],[1,2], 'd', markersize=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this problem, we are looking for a model that captures the ***functional relationship*** between these two points.\n",
    "\n",
    "Using **linear regression** our *hypothesis* is that such ***functional relationship*** can be **modeled** as a **line**, represented as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= f(x) \\\\\n",
    "  &= \\beta_0 + \\beta_{1}x_{1} \\\\\n",
    "  &= \\beta_0 1 + \\beta_{1}x_{1} \\\\\n",
    "  &= \\beta_0 x_0 + \\beta_{1}x_{1} \\quad  \\text{where $x_0=1$}\\\\\n",
    "  &= \\sum_{i=0}^1 \\beta_{i}x_{i} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> Do you remember $ y = m x + b$ ?... where $m$ is known as the *slope* and $b$ is the *itercept* \n",
    "\n",
    "We can rewrite this expression using **vector notation**, where vectors may be *geometric vectors* or *abstract members* of **vector spaces**.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= f(\\mathbf{x};\\boldsymbol{\\beta}) \\\\\n",
    "  &= \\langle  \\boldsymbol{\\beta} , \\mathbf{x} \\rangle \\\\\n",
    "  &= \\boldsymbol{\\beta}\\cdot \\mathbf{x} \\\\ \n",
    "  &= \\boldsymbol{\\beta}^{T}\\mathbf{x} \\\\ \n",
    "  &= [\\beta_0,\\beta_1] \\begin{bmatrix} x_0\\\\x_1 \\end{bmatrix}  \\\\\n",
    "  &= \\beta_0 x_0 + \\beta_{1}x_{1} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In our problem, since both points *live* in the same line (**linear relationship**), they both should satisfy (*obey*) the same linear model. So let's compute the $\\beta$ parameters, which are common for our data points\n",
    "\n",
    "point | $x_0$ | $x_1$| $y$ | Model: $ y = \\beta_{0} x_{0} + \\beta_1 x_1 $\n",
    "-|-|-|-|-\n",
    "$p_1$ |1|1|1| $1 = \\beta_0 (1) + \\beta_{1} (1)$  \n",
    "$p_2$ |1|3|2| $2 = \\beta_0 (1) + \\beta_{1} (3)$ \n",
    "\n",
    "These two equations are actually a ***$2\\times 2$ system of linear algebraic equations***, i.e., ** $2$ equations in $2$ unknowns**,\n",
    "which we have to solve, in order to figure out what is the line that *passes* through the points.\n",
    "\n",
    "$$\n",
    "\\beta_0 (1) + \\beta_{1} (1) = 1 \\\\\n",
    "\\beta_0 (1) + \\beta_{1} (3) = 2 \n",
    "$$ \n",
    "\n",
    "Thus, the system may be written as the matrix equation $\\mathbf{A} \\cdot \\boldsymbol{\\beta} = \\mathbf{y} $, where\n",
    "\n",
    "\n",
    "$\n",
    "\\mathbf{A}= \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$, \n",
    "$\n",
    "\\boldsymbol{\\beta} = \n",
    "\\begin{bmatrix} \n",
    "\\beta_{0}\\\\ \\beta_{1}\n",
    "\\end{bmatrix} \n",
    "$,\n",
    "$\n",
    "\\mathbf{y}= \n",
    "\\begin{bmatrix} \n",
    "1\\\\ 2\n",
    "\\end{bmatrix} \n",
    "$\n",
    "\n",
    "thus, $\\mathbf{A} \\cdot \\boldsymbol{\\beta} = \\mathbf{y}$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix} \n",
    "\\beta_{0}\\\\ \\beta_{1}\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1\\\\ 2\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Can you see the **linear combination** of the **column vectors** of our system ?\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 1 \n",
    "\\end{bmatrix}\n",
    "\\beta_0\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 3 \n",
    "\\end{bmatrix}\n",
    "\\beta_1\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1\\\\ 2\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now, we are just coming to the point where we have several **algebraic equations**, so we need **matrices**.\n",
    "> Do you remember $ \\mathbf{A \\cdot x} = \\mathbf{b}$ and how to solve it? ... where $\\mathbf{A}$, $\\mathbf{b}$ are *known*, and $ \\mathbf{x}$ is *unknown*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Algebra breviary\n",
    "---\n",
    "\n",
    "---\n",
    "# Linear Equations\n",
    "---\n",
    "\n",
    "A **fundamental problem** that commonly arises in all **mathematical sciences** is that of ***analyzing and solving*** $m$ algebraic equations in $n$ unknowns.\n",
    "\n",
    "The **problem** is to determine, if possible, a common solution for a system. In general, we have $m$ ***linear algebraic equations*** in $n$ unknowns\n",
    "\n",
    "$$\n",
    "a_{1,1} x_1 + a_{1,2} x_2 + \\ldots + a_{1,n} x_n = b_1 ,\\\\\n",
    "a_{2,1} x_1 + a_{2,2} x_2 + \\ldots + a_{2,n} x_n = b_2 ,\\\\\n",
    "\\vdots \\\\\n",
    "a_{m,1} x_1 + a_{m,2} x_2 + \\ldots + a_{m,n} x_n = b_m \n",
    "$$\n",
    "\n",
    "where the $x_i$'s are the *unknown variables* and the $a_{ij}$'s, called the ***coefficients***, and the $b_i$'s, the ***right-hand side of the system***, are *known constants*.\n",
    "\n",
    "---\n",
    "The system may be rewritten as $\\mathbf{A \\cdot x} = \\mathbf{b}$, where\n",
    "\n",
    "$\n",
    "% matrix, pmatrix, bmatrix, Bmatrix, vmatrix, Vmatrix\n",
    "\\mathbf{A}= \n",
    " \\begin{pmatrix} %smallmatrix\n",
    "  a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\n",
    "  a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  a_{m,1} & a_{m,2} & \\cdots & a_{m,n} \n",
    " \\end{pmatrix}\n",
    "$,\n",
    "$\n",
    "\\mathbf{x}= \n",
    " \\begin{pmatrix} %smallmatrix\n",
    "  x_{1}\\\\ x_{2}\\\\ \\vdots\\\\ x_{n} \n",
    " \\end{pmatrix} \n",
    " $,\n",
    " $\n",
    "\\mathbf{b}= \n",
    " \\begin{pmatrix} %smallmatrix\n",
    "  b_{1}\\\\ b_{2}\\\\ \\vdots\\\\ b_{m} \n",
    " \\end{pmatrix}\n",
    "$\n",
    "\n",
    "thus, $\\mathbf{A \\cdot x} = \\mathbf{b}$ becomes\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} %smallmatrix\n",
    "a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\n",
    "a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "a_{m,1} & a_{m,2} & \\cdots & a_{m,n} \n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix} %smallmatrix\n",
    "x_{1}\\\\ x_{2}\\\\ \\vdots\\\\ x_{n} \n",
    "\\end{pmatrix} \n",
    "= \n",
    "\\begin{pmatrix} %smallmatrix\n",
    "b_{1}\\\\ b_{2}\\\\ \\vdots\\\\ b_{m} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\mathbf{A}$ is called the **coefficient matrix** for the system.\n",
    "\n",
    "\n",
    "---\n",
    "# Solving linear equations systems\n",
    "There are exactly *three possibilities* for the ***set of solutions*** for any such system:\n",
    "+ **UNIQUE SOLUTION**. There is one and only one set of values for the $x_i$'s that satisfies all equations simultaneously.\n",
    "+ **INFINITELY MANY SOLUTIONS**. There are infinitely many different sets of values for the $x_i$'s that satisfy all equations simultaneously. If a system has more than one solution, then it has infinitely many solutions. \n",
    "+ **NO SOLUTION**. There is no set of values for the $x_i$'s that satisfies all equations simultaneously, i.e., the solution set is empty.\n",
    "\n",
    "> An important part of the job in dealing with **linear systems** is to ***verify*** which one of these three possibilities is true. The other important part of the task is to ***compute*** the **solution**.\n",
    "\n",
    "\n",
    "\n",
    "# Solving linear equations systems\n",
    "**Gaussian elimination** (a.k.a.  *row reduction*) is an algorithm for successively *eliminating* **unknowns**, eventually *arriving* at a system that is *easily solvable* \n",
    "+ it systematically ***transforms*** one system into another simpler system\n",
    "+ two systems are called **equivalent** if they possess equal solution sets\n",
    "\n",
    "The elimination process relies on **three simple operations**. \n",
    "\n",
    "Lets write the system \n",
    "$$\n",
    "a_{1,1} x_1 + a_{1,2} x_2 + \\ldots + a_{1,n} x_n = b_1 ,\\\\\n",
    "a_{2,1} x_1 + a_{2,2} x_2 + \\ldots + a_{2,n} x_n = b_2 ,\\\\\n",
    "\\vdots \\\\\n",
    "a_{m,1} x_1 + a_{m,2} x_2 + \\ldots + a_{m,n} x_n = b_m \n",
    "$$\n",
    "\n",
    "as\n",
    "$$\n",
    "\\mathcal{S} = \n",
    "\\begin{Bmatrix} %smallmatrix\n",
    "E_{1}\\\\ E_{2}\\\\ \\vdots\\\\ E_{m} \n",
    "\\end{Bmatrix} \n",
    "$$\n",
    "\n",
    "where the $k$th equation is given by $E_k:  a_{k,1}x_1 + a_{k,2}x_2 + \\ldots + a_{k,n}x_n = b_k$\n",
    "\n",
    "For such a linear system $\\mathcal{S}$, each of the following three **elementary operations** results in an equivalent system $\\mathcal{S'}$:\n",
    "\n",
    "1\\. ***Interchange*** the $i$th and $j$th equations. That is, if\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\mathcal{S} &= \n",
    "\\begin{Bmatrix} %smallmatrix\n",
    "E_{1}\\\\ \\vdots \\\\ E_{i} \\\\ \\vdots \\\\ E_{j} \\\\ \\vdots \\\\ E_{m} \n",
    "\\end{Bmatrix} \n",
    "\\quad  \\text{then}\n",
    "\\end{align*}\n",
    "$\n",
    "$\n",
    "\\mathcal{S'} = \n",
    "\\begin{Bmatrix} %smallmatrix\n",
    "E_{1}\\\\ \\vdots \\\\ E_{j} \\\\ \\vdots \\\\ E_{i} \\\\ \\vdots \\\\ E_{m} \n",
    "\\end{Bmatrix} \n",
    "$\n",
    "\n",
    "2\\. ***Replace*** the $i$th equation by a nonzero scalar multiple of itself. That is, if\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\mathcal{S} &= \n",
    "\\begin{Bmatrix} %smallmatrix\n",
    "E_{1}\\\\ \\vdots \\\\ E_{i} \\\\  \\vdots \\\\ E_{m} \n",
    "\\end{Bmatrix} \n",
    "\\quad  \\text{then}\n",
    "\\end{align*}\n",
    "$\n",
    "$\n",
    "\\mathcal{S'} = \n",
    "\\begin{Bmatrix} %smallmatrix\n",
    "E_{1}\\\\ \\vdots \\\\ \\alpha E_{i} \\\\ \\vdots \\\\ E_{m} \n",
    "\\end{Bmatrix} \n",
    "$\n",
    "\n",
    "3\\. ***Replace*** the $i$th equation by a combination of itself plus a nonzero scalar multiple of the $j$th equation. That is, if\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\mathcal{S} &= \n",
    "\\begin{Bmatrix} %smallmatrix\n",
    "E_{1}\\\\ \\vdots \\\\ E_{i} \\\\ \\vdots \\\\ E_{j} \\\\ \\vdots \\\\ E_{m} \n",
    "\\end{Bmatrix} \n",
    "\\quad  \\text{then}\n",
    "\\end{align*}\n",
    "$\n",
    "$\n",
    "\\mathcal{S'} = \n",
    "\\begin{Bmatrix} %smallmatrix\n",
    "E_{1}\\\\ \\vdots \\\\ E_{i} + \\alpha E_{j}\\\\ \\vdots \\\\ E_{j} \\\\ \\vdots \\\\ E_{m} \n",
    "\\end{Bmatrix} \n",
    "$\n",
    "\n",
    "> **NOTE**: Providing explanations for why each of these operations cannot change the solution set is left up to you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going back from our linear algebra breviary... \n",
    "\n",
    "The matrix equation we are interested in is $\\mathbf{A} \\cdot \\boldsymbol{\\beta} = \\mathbf{y} $, and the system we are dealing with is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix} \n",
    "\\beta_{0}\\\\ \\beta_{1}\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1\\\\ 2\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "In order to reduce this system we need to apply the **elementary operations**. \n",
    "\n",
    "However, when manipulating the system, there is no reason to write down the $\\beta_i$'s symbols and \"$=$\" at *each step* since we are only manipulating the known values; therefore, such symbols are discarded.\n",
    "\n",
    "\n",
    "We can write the **coefficient matrix** along with the numbers from the **right-hand side** of the system, as en entire array called the ***augmented coefficient matrix***. \n",
    "\n",
    "$$\n",
    "\\mathcal{S}=  [ \\mathbf{A} \\; | \\; \\mathbf{y} ]\n",
    "$$\n",
    "\n",
    "where the line emphasizes where the \"$=$\" appeared.\n",
    "\n",
    "$\n",
    "\\mathcal{S}= \n",
    "\\begin{bmatrix}\n",
    "\\begin{array}{cc|c}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 3 & 2\\\\\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\mathcal{S'}= \n",
    "\\begin{bmatrix}\n",
    "\\begin{array}{cc|c}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 3 & 2\\\\\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\xrightarrow{E_2 \\leftarrow E_2 - E_1 }$\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\begin{array}{cc|c}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 2 & 1\\\\\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\mathcal{S'}= \n",
    "\\begin{bmatrix}\n",
    "\\begin{array}{cc|c}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 2 & 1\\\\\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\xrightarrow{E_2 \\leftarrow \\alpha E_2}$\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\begin{array}{cc|c}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 1 & \\frac{1}{2}\\\\\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\mathcal{S'}= \n",
    "\\begin{bmatrix}\n",
    "\\begin{array}{cc|c}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 1 & \\frac{1}{2}\\\\\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\xrightarrow{E_1 \\leftarrow E_1 - E_2}$\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\begin{array}{cc|c}\n",
    "1 & 0 & \\frac{1}{2}\\\\\n",
    "0 & 1 & \\frac{1}{2}\\\\\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The final array represents the system\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} %smallmatrix\n",
    "1 & 0  \\\\\n",
    "0 & 1  \n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix} \n",
    "\\beta_{0}\\\\ \\beta_{1}\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\\frac{1}{2}\\\\ \\frac{1}{2} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "where $ \\begin{bmatrix} 1 & 0\\\\ 0 & 1  \\end{bmatrix}$ is know as the $2\\times2$ **identity matrix**, which satisfies\n",
    "\n",
    "1. $ \\mathbf{I}_m \\cdot \\mathbf{A} = \\mathbf{A} \\cdot \\mathbf{I}_n =  \\mathbf{A} $\n",
    "2. $ \\mathbf{I} \\cdot \\mathbf{I}^{-1} = \\mathbf{I}^{-1} \\cdot \\mathbf{I} =\n",
    "\\mathbf{I}\n",
    "$\n",
    "\n",
    "So, our system becomes \n",
    "\n",
    "$$\n",
    "\\mathbf{I} \\cdot \\boldsymbol{\\beta} = \\boldsymbol{\\beta} =\n",
    "\\begin{bmatrix} \n",
    "\\frac{1}{2}\\\\ \\frac{1}{2} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "What we actually just did, \n",
    "was to solve the matrix equation $\\mathbf{A \\cdot x} = \\mathbf{b}$,\n",
    "by finding:\n",
    "\n",
    "$$ \\mathbf{x} = \\mathbf{A}^{-1} \\cdot \\mathbf{b} $$\n",
    "\n",
    "Finally,  our initial *hypothesis* was that the ***functional relationship*** between our data points could  be **modeled** as a **line**, represented as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= f(x) \\\\\n",
    "  &= \\beta_0 x_0 + \\beta_{1}x_{1} \\quad  \\text{where $x_0=1$}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And now, we know that \n",
    "$\n",
    "\\boldsymbol{\\beta} =\n",
    "\\begin{bmatrix} \n",
    "\\frac{1}{2}\\\\ \\frac{1}{2} \n",
    "\\end{bmatrix} \n",
    "$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= f(x) \\\\\n",
    "  &= \\frac{1}{2}  x_0 + \\frac{1}{2} x_{1} \\quad  \\text{where $x_0=1$}\\\\\n",
    "  \\therefore \\\\\n",
    "y &= \\frac{1}{2} + \\frac{1}{2} x_{1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "point | $x_0$ | $x_1$| $y$ | Fitted model: $ y = \\frac{1}{2} + \\frac{1}{2} x_{1}$\n",
    "-|-|-|-|-\n",
    "$p_1$ |1|1|1| $1 = \\frac{1}{2} (1) + \\frac{1}{2} (1)$  \n",
    "$p_2$ |1|3|2| $2 = \\frac{1}{2} (1) + \\frac{1}{2} (3)$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.5],\n",
       "        [ 0.5]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "from numpy import array, matrix, transpose, sqrt, dot\n",
    "from numpy.linalg import  inv, norm, det, eig, svd \n",
    "\n",
    "v1 = matrix('1, 1; 1 ,3') #v1 = array([[1, 1], [1 ,3]]) \n",
    "v2 = matrix('1; 2')       #v2 = array([[6], [1]])\n",
    "\n",
    "dot(inv(v1), v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting model coefficients\n",
    "$$y = f(x) = 0.5 + 0.5 x$$\n",
    "\n",
    "How do we interpret the **x coefficient** (0.5)?\n",
    "- a **unit** increase in $x$ is **associated** with a **half unit** **increase** in $y$.\n",
    "- e.g., **an additional** $\\$1,000$ spent on $x$-type ads is associated with an **increase** in sales of 500 items.\n",
    "\n",
    "Important notes:\n",
    "- This is a statement of **association**, not **causation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b4487a160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# (1,1), (3,2)\n",
    "x = np.array([1,3])\n",
    "y = np.array([1,2])\n",
    "plt.plot(x,y, markersize=10)\n",
    "plt.plot([0, 1], [1,1], color='grey', linewidth=1,linestyle='dashed')\n",
    "plt.plot([1,1],[0,1], color='grey', linewidth=1,linestyle='dashed')\n",
    "plt.plot([0, 3], [2,2], color='grey', linewidth=1,linestyle='dashed')\n",
    "plt.plot([3,3],[0,2], color='grey', linewidth=1,linestyle='dashed')\n",
    "plt.plot([0, 2], [1.5,1.5], color='grey', linewidth=3,linestyle='dashdot')\n",
    "plt.plot([2,2],[0,1.5], color='grey', linewidth=3,linestyle='dashdot')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "### Approximation theory (*review*)\n",
    "\n",
    "What if a system of equations have no solution?\n",
    "\n",
    "$$ \n",
    "\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "\n",
    "**Approximation theory** is concerned with *how functions can best be approximated with simpler functions*.\n",
    "+ the matrix equation we are interested in\n",
    "$$ \n",
    "\\mathbf{A}^{T}\\cdot \\mathbf{A} \\cdot \\hat{\\mathbf{x}} =\n",
    "\\mathbf{A}^{T} \\cdot \\mathbf{b} \n",
    "$$\n",
    "+ A closely related topic is the approximation of functions by **generalized Fourier series** \n",
    "    + approximations based on series of **orthogonal vectors (*polynomials*)**\n",
    "    + **Fourier** and **Karhunen–Loeve** transforms\n",
    "\n",
    "### Case study: Advertising Data\n",
    "\n",
    "We'll take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!\n",
    "\n",
    "*Adapted from Chapter 3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments or Questions?\n",
    "- Email: <cross224@hotmail.com>\n",
    "- GitHub: [@carap](https://github.com/carap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
